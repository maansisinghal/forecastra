{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9174a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Paths: ['D:/Forecastra Accelerator/calendar.csv', 'D:/Forecastra Accelerator/sales_train_validation.csv']\n",
      "Functions: [{'name': 'find_missing_dates', 'params': {'date_freq': 'D'}}, {'name': 'validate_minimum_timeperiod_value', 'params': {'min_value': 100}}]\n",
      "Outputs: {'calendar_path': 'D:/Forecastra Accelerator/dataset/calendar_data.csv', 'sales_train_validation_path': 'D:/Forecastra Accelerator/dataset/forecastra_input_data.csv'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"D:/Forecastra Accelerator/dataset/.vscode/read_config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Access different sections of the configuration\n",
    "input_paths = config[\"inputs\"][\"paths\"]\n",
    "functions = config[\"functions\"]\n",
    "outputs = config[\"outputs\"]\n",
    "\n",
    "# Print some example values\n",
    "print(\"Input Paths:\", input_paths)\n",
    "print(\"Functions:\", functions)\n",
    "print(\"Outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e4466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b892c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"inputs\": {\n",
    "        \"paths\": [\n",
    "            \"D:/Forecastra Accelerator/dataset/calendar_data.csv\",\n",
    "            \"D:/Forecastra Accelerator/dataset/forecastra_input_data.csv\"\n",
    "        ]\n",
    "    },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"outputs\": {\n",
    "        \"calendar_path\": \"D:/Forecastra Accelerator/dataset/calendar_data_config.csv\",\n",
    "        \"sales_train_validation_path\": \"D:/Forecastra Accelerator/dataset/forecastra_input_data_config.csv\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed8d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    #print(input_paths)\n",
    "    for path in input_paths:\n",
    "        print(path)\n",
    "        #print(path.values())\n",
    "        #print(path.keys()) \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"inputs\": {\n",
    "        \"paths\": [\n",
    "            {\n",
    "            \"calendar\": \"D:/Forecastra Accelerator/dataset/calendar_data.csv\"\n",
    "            },\n",
    "            {\n",
    "            \"time series\": \"D:/Forecastra Accelerator/dataset/forecastra_input_data.csv\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"validate_date_format\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"find_missing_dates\",\n",
    "            \"params\": {\n",
    "                \"date_freq\": \"D\",\n",
    "                \"apply_to_dataset\": \"sales_train_validation_path\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"validate_minimum_timeperiod_value\",\n",
    "            \"params\": {\n",
    "                \"min_value\": 3000\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"outputs\": {\n",
    "        \"calendar\": \"D:/Forecastra Accelerator/dataset/calendar_data_config.csv\",\n",
    "        \"time series\": \"D:/Forecastra Accelerator/dataset/forecastra_input_data_config.csv\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd470ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                     result \u001b[38;5;241m=\u001b[39m validate_minimum_timeperiod_value(df, min_value) \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Call the function with the path to your config file\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mprocess_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/Forecastra Accelerator/dataset/.vscode/read_config.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mprocess_config\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(config_file)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Access different sections of the configuration\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m input_paths \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m functions \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'inputs'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def process_config(config_path):\n",
    "    # Load JSON configuration from file\n",
    "    with open(\"D:/Forecastra Accelerator/dataset/.vscode/read_config.json\", \"r\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    # Access different sections of the configuration\n",
    "    input_paths = config[\"inputs\"][\"paths\"]\n",
    "    functions = config[\"functions\"]\n",
    "    outputs = config[\"outputs\"]\n",
    "    \n",
    "    # Calling functions\n",
    "    for path_dict in input_paths:\n",
    "        for key, path in path_dict.items():\n",
    "\n",
    "            df = pd.read_csv(path)\n",
    "            print(df)\n",
    "            for function in functions:\n",
    "                function_name = function[\"name\"]\n",
    "\n",
    "                if function_name == \"validate_date_format\":\n",
    "                    validate_date_format(df)\n",
    "\n",
    "                if function_name == \"find_missing_dates\":\n",
    "                    params = function[\"params\"]\n",
    "                    date_freq = params[\"date_freq\"]\n",
    "                    result = find_missing_dates(df, date_freq)\n",
    "\n",
    "                elif function_name == \"validate_minimum_timeperiod_value\":\n",
    "                    params = function[\"params\"]\n",
    "                    min_value = params[\"min_value\"]\n",
    "                    result = validate_minimum_timeperiod_value(df, min_value) \n",
    "\n",
    "# Call the function with the path to your config file\n",
    "process_config(\"D:/Forecastra Accelerator/dataset/.vscode/read_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de091c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "'''     print(dir_path)\n",
    "        print(file_name)\n",
    "        print(file_format)\n",
    "        print(functions)\n",
    "        print(file_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e3224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidationBase(ABC):\n",
    "    def __init__(self, config_path):\n",
    "        self.config_path = config_path\n",
    "        self.load_config()\n",
    "        \n",
    "    def load_config(self):\n",
    "        # Load JSON configuration from file\n",
    "        with open(self.config_path, \"r\") as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass\n",
    "    \n",
    "#Generating alert for invalid date format\n",
    "class Alert:\n",
    "    \n",
    "    def __init__(self, timestamp, stage, substage=None, message=\"\", alert_type=\"error\"):\n",
    "        self.timestamp = timestamp\n",
    "        self.stage = stage\n",
    "        self.substage = substage\n",
    "        self.message = message\n",
    "        self.alert_type = alert_type \n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Timestamp: {self.timestamp}\\n\"\n",
    "            f\"Stage: {self.stage}\\n\"\n",
    "            f\"Substage: {self.substage}\\n\"\n",
    "            f\"Message: {self.message}\\n\"\n",
    "            f\"Type: {self.alert_type}\"\n",
    "        )\n",
    "    \n",
    "class DataValidation(DataValidationBase):\n",
    "    def run(self):\n",
    "        input_paths = self.config[\"input\"]\n",
    "        functions = self.config[\"function\"]\n",
    "        outputs = self.config[\"output\"]\n",
    "        dir_path = input_paths[\"dir\"]\n",
    "        file_format = input_paths[\"fileFormat\"]\n",
    "        \n",
    "        for path_dict in input_paths[\"file\"]:\n",
    "            for key, file_name in path_dict.items():\n",
    "                file_path = f\"{dir_path}/{file_name}.{file_format}\"\n",
    "                print(f\"Input File '{key}' - File path:\", file_path)\n",
    "                \n",
    "                dataframe = pd.read_csv(file_path)\n",
    "                \n",
    "                for function in functions:\n",
    "                    function_name = function[\"name\"]\n",
    "                    params = function.get(\"params\", {})\n",
    "                    \n",
    "                    # Dynamically call the function by its name if it exists in the class\n",
    "                    if hasattr(self, function_name):\n",
    "                        func = getattr(self, function_name)\n",
    "                        func(dataframe, **params)\n",
    "                \n",
    "    def creating_unique_key(self, dataframe, key_columns):\n",
    "        if len(key_columns) == 1:\n",
    "            # Rename the single selected column as 'unique_id'\n",
    "            dataframe.rename(columns={key_columns[0]: 'unique_id'}, inplace=True)\n",
    "            key_series = dataframe['unique_id']\n",
    "        else:\n",
    "            # Combine selected columns into a new 'unique_id' column\n",
    "            key_series = dataframe[key_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
    "\n",
    "        return key_series\n",
    "                \n",
    "                \n",
    "    def validate_date_format(self, dataframe):\n",
    "        \n",
    "        if not pd.api.types.is_datetime64_any_dtype(dataframe['date']):\n",
    "            for date_str in dataframe['date']:\n",
    "                try:\n",
    "                    datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                except ValueError as e:\n",
    "                    alert = Alert(\n",
    "                        timestamp=datetime.now(),\n",
    "                        stage=\"Data Validation\",\n",
    "                        message=f\"Invalid date format: {date_str}. Please use the format YYYY-MM-DD.\"\n",
    "                    )\n",
    "                    print(alert)\n",
    "                    raise ValueError(alert)\n",
    "\n",
    "\n",
    "    def find_missing_dates(self, dataframe, date_freq):\n",
    "        # Checking for missing dates\n",
    "\n",
    "        # Sort the dataframe by 'date'\n",
    "        dataframe['date'] = pd.to_datetime(dataframe['date']).astype('datetime64[ns]')\n",
    "        df = dataframe.sort_values('date')\n",
    "\n",
    "        # Generate a range of dates covering the desired time period\n",
    "        start_date = df['date'].min()\n",
    "        end_date = df['date'].max()\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=date_freq)\n",
    "\n",
    "        # Find missing dates\n",
    "        missing_dates = date_range[~date_range.isin(df['date'])]\n",
    "\n",
    "        # Generate warnings for missing dates\n",
    "        for missing_date in missing_dates:\n",
    "            warning_message = f\"Warning: Data is missing for date {missing_date.date()}\"\n",
    "            warnings.warn(warning_message, UserWarning)\n",
    "\n",
    "    \n",
    "    def validate_minimum_timeperiod_value(self, dataframe, min_time_period):\n",
    "        #User to enter minimum time period value\n",
    "\n",
    "        if dataframe['date'].max() - dataframe['date'].min() < pd.Timedelta(days=min_time_period):\n",
    "            warning_message = f\"Warning: Data does not have minimum time period of {min_time_period} days.\"\n",
    "            warnings.warn(warning_message, UserWarning)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    config_path = \"D:/Forecastra Accelerator/dataset/.vscode/read_config.json\"\n",
    "    data_validator = DataValidation(config_path)\n",
    "    data_validator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b5e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a37486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9472c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidationBase(ABC):\n",
    "    def __init__(self, config_path):\n",
    "        self.config_path = config_path\n",
    "        self.load_config()\n",
    "        \n",
    "    def load_config(self):\n",
    "        # Load JSON configuration from file\n",
    "        with open(self.config_path, \"r\") as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass\n",
    "    \n",
    "#Generating alert for invalid date format\n",
    "class Alert:\n",
    "    \n",
    "    def __init__(self, timestamp, stage, substage=None, message=\"\", alert_type=\"error\"):\n",
    "        self.timestamp = timestamp\n",
    "        self.stage = stage\n",
    "        self.substage = substage\n",
    "        self.message = message\n",
    "        self.alert_type = alert_type \n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Timestamp: {self.timestamp}\\n\"\n",
    "            f\"Stage: {self.stage}\\n\"\n",
    "            f\"Substage: {self.substage}\\n\"\n",
    "            f\"Message: {self.message}\\n\"\n",
    "            f\"Type: {self.alert_type}\"\n",
    "        )\n",
    "    \n",
    "class DataValidation(DataValidationBase):\n",
    "    def run(self):\n",
    "        input_paths = self.config[\"input\"]\n",
    "        functions = self.config[\"function\"]\n",
    "        outputs = self.config[\"output\"]\n",
    "        dir_path = input_paths[\"dir\"]\n",
    "        file_format = input_paths[\"fileFormat\"]\n",
    "        \n",
    "        for file_entry in input_paths[\"file\"]:\n",
    "            # Extract the input file name\n",
    "            file_name, file_functions = list(file_entry.items())[0]\n",
    "            \n",
    "            \n",
    "            file_path = f\"{dir_path}/{file_functions}.{file_format}\"\n",
    "            print(f\"Input File '{file_name}' - File path:\", file_path)\n",
    "\n",
    "            dataframe = pd.read_csv(file_path)\n",
    "\n",
    "            for function in file_functions:\n",
    "                params = {}\n",
    "\n",
    "                # Dynamically call the function by its name if it exists in the class\n",
    "                if hasattr(self, function):\n",
    "                    func = getattr(self, function)\n",
    "                    if function_name in functions:\n",
    "                        params = functions[function].get(\"params\", {})\n",
    "                    func(dataframe, **params)\n",
    "                    \n",
    "                    # Check if the result is a list of Alerts or Warnings\n",
    "                    if isinstance(result, list):\n",
    "                        for item in result:\n",
    "                            if isinstance(item, Alert):\n",
    "                                alerts.append(item)\n",
    "                            elif isinstance(item, Warning):\n",
    "                                warnings.append(item)\n",
    "                    \n",
    "        return alerts, warnings\n",
    "    \n",
    "    alerts = []\n",
    "    warnings = []\n",
    "\n",
    "    def creating_unique_key(self, dataframe, key_columns):\n",
    "        if len(key_columns) == 1:\n",
    "            # Rename the single selected column as 'unique_id'\n",
    "            dataframe.rename(columns={key_columns[0]: 'unique_id'}, inplace=True)\n",
    "            key_series = dataframe['unique_id']\n",
    "        else:\n",
    "            # Combine selected columns into a new 'unique_id' column\n",
    "            key_series = dataframe[key_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
    "\n",
    "        return key_series\n",
    "                \n",
    "                \n",
    "    def validate_date_format(self, dataframe):\n",
    "        \n",
    "        if not pd.api.types.is_datetime64_any_dtype(dataframe['date']):\n",
    "            for date_str in dataframe['date']:\n",
    "                try:\n",
    "                    datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                except ValueError as e:\n",
    "                    alert = Alert(\n",
    "                        timestamp=datetime.now(),\n",
    "                        stage=\"Data Validation\",\n",
    "                        message=f\"Invalid date format: {date_str}. Please use the format YYYY-MM-DD.\"\n",
    "                    )\n",
    "                    alerts.append(alert)\n",
    "                    raise ValueError(alert)\n",
    "        return alerts\n",
    "\n",
    "\n",
    "    def find_missing_dates(self, dataframe, date_freq = 'D'):\n",
    "    \n",
    "        # Checking for missing dates\n",
    "\n",
    "        # Sort the dataframe by 'date'\n",
    "        dataframe['date'] = pd.to_datetime(dataframe['date']).astype('datetime64[ns]')\n",
    "        df = dataframe.sort_values('date')\n",
    "\n",
    "        # Generate a range of dates covering the desired time period\n",
    "        start_date = df['date'].min()\n",
    "        end_date = df['date'].max()\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=date_freq)\n",
    "\n",
    "        # Find missing dates\n",
    "        missing_dates = date_range[~date_range.isin(df['date'])]\n",
    "\n",
    "        # Generate warnings for missing dates\n",
    "        for missing_date in missing_dates:\n",
    "            warning = Warning(\n",
    "                timestamp=datetime.now(),\n",
    "                stage=\"Data Validation\",\n",
    "                message=f\"Warning: Data is missing for date {missing_date.date()}\"\n",
    "            )\n",
    "            warnings.append(warning)\n",
    "            warnings.warn(message, UserWarning)\n",
    "        return warnings\n",
    "\n",
    "    \n",
    "    def validate_minimum_timeperiod_value(self, dataframe, min_time_period = 100):\n",
    "        \n",
    "        #User to enter minimum time period value\n",
    "\n",
    "        if dataframe['date'].max() - dataframe['date'].min() < pd.Timedelta(days=min_time_period):\n",
    "            warning = Warning(\n",
    "                timestamp=datetime.now(),\n",
    "                stage=\"Data Validation\",\n",
    "                message=f\"Warning: Data does not have minimum time period of {min_time_period} days.\"\n",
    "            )\n",
    "            warnings.append(warning)\n",
    "            warnings.warn(message, UserWarning)\n",
    "        return warnings        \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    config_path = \"D:/Forecastra Accelerator/dataset/.vscode/read_config.json\"\n",
    "    data_validator = DataValidation(config_path)\n",
    "    alerts, warnings = data_validator.run()\n",
    "\n",
    "    print(\"Alerts:\")\n",
    "    for alert in alerts:\n",
    "        print(alert)\n",
    "\n",
    "    print(\"\\nWarnings:\")\n",
    "    for warning in warnings:\n",
    "        print(warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d7c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3a841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c3a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36666ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ad088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8914be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcc9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage(self):\n",
    "    input_paths = self.config[\"input\"]\n",
    "    outputs = self.config[\"output\"]\n",
    "    dir_path = input_paths[\"dir\"]\n",
    "    file_format = input_paths[\"fileFormat\"]\n",
    "\n",
    "    for file_entry in input_paths[\"file\"]:\n",
    "        # Extract the input file name and associated functions\n",
    "        file_name = list(file_entry.values())[0]\n",
    "        print(file_name)\n",
    "        # function_name = list(file_entry.values())[1]\n",
    "        function_name = file_entry.get(\"functions\", [])\n",
    "\n",
    "        file_path = f\"{dir_path}/{file_name}.{file_format}\"\n",
    "        print(f\"Input File '{file_name}' - File path:\", file_path)\n",
    "        print(function_name)\n",
    "\n",
    "        dataframe = pd.read_csv(file_path)\n",
    "\n",
    "        for function_entry in self.config[\"function\"]:\n",
    "            function_name = function_entry[\"name\"]\n",
    "            print(f\"Function Name: {function_name}\")\n",
    "\n",
    "            # If \"params\" is present, you can access its values like this\n",
    "            params = function_entry.get(\"params\", {})\n",
    "            print(f\"Parameters: {params}\")\n",
    "\n",
    "            # Check if the function exists in the DataValidation class\n",
    "            if hasattr(self, function_name):\n",
    "                func = getattr(self, function_name)\n",
    "                # Call the function with the DataFrame and parameters\n",
    "                result_dataframe = func(dataframe, **params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ed9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8634dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe5febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca864d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268db5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f220ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61426a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-02-2011</td>\n",
       "      <td>SuperBowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14-02-2011</td>\n",
       "      <td>ValentinesDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21-02-2011</td>\n",
       "      <td>PresidentsDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09-03-2011</td>\n",
       "      <td>LentStart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-03-2011</td>\n",
       "      <td>LentWeek2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17-03-2011</td>\n",
       "      <td>StPatricksDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20-03-2011</td>\n",
       "      <td>Purim End</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24-04-2011</td>\n",
       "      <td>OrthodoxEaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26-04-2011</td>\n",
       "      <td>Pesach End</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>05-05-2011</td>\n",
       "      <td>Cinco De Mayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>08-05-2011</td>\n",
       "      <td>Mother's day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date           event\n",
       "0   06-02-2011       SuperBowl\n",
       "1   14-02-2011   ValentinesDay\n",
       "2   21-02-2011   PresidentsDay\n",
       "3   09-03-2011       LentStart\n",
       "4   16-03-2011       LentWeek2\n",
       "5   17-03-2011   StPatricksDay\n",
       "6   20-03-2011       Purim End\n",
       "7   24-04-2011  OrthodoxEaster\n",
       "8   26-04-2011      Pesach End\n",
       "9   05-05-2011   Cinco De Mayo\n",
       "10  08-05-2011    Mother's day"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = 'calendar_data_config.parquet'\n",
    "\n",
    "# Use pandas to read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "df\n",
    "\n",
    "# Now, you can work with the DataFrame 'df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22b688c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_001_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_002_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_003_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>5</td>\n",
       "      <td>HOBBIES_1_004_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_005_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_006_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_007_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>2</td>\n",
       "      <td>HOBBIES_1_008_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_009_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_010_CA_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HOBBIES_1_011</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_011_CA_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          item_id store_id       date  sales           unique_id\n",
       "0   HOBBIES_1_001     CA_1 2015-04-24      0  HOBBIES_1_001_CA_1\n",
       "1   HOBBIES_1_002     CA_1 2015-04-24      0  HOBBIES_1_002_CA_1\n",
       "2   HOBBIES_1_003     CA_1 2015-04-24      1  HOBBIES_1_003_CA_1\n",
       "3   HOBBIES_1_004     CA_1 2015-04-24      5  HOBBIES_1_004_CA_1\n",
       "4   HOBBIES_1_005     CA_1 2015-04-24      0  HOBBIES_1_005_CA_1\n",
       "5   HOBBIES_1_006     CA_1 2015-04-24      0  HOBBIES_1_006_CA_1\n",
       "6   HOBBIES_1_007     CA_1 2015-04-24      0  HOBBIES_1_007_CA_1\n",
       "7   HOBBIES_1_008     CA_1 2015-04-24      2  HOBBIES_1_008_CA_1\n",
       "8   HOBBIES_1_009     CA_1 2015-04-24      0  HOBBIES_1_009_CA_1\n",
       "9   HOBBIES_1_010     CA_1 2015-04-24      1  HOBBIES_1_010_CA_1\n",
       "10  HOBBIES_1_011     CA_1 2015-04-24      0  HOBBIES_1_011_CA_1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = 'forecastra_input_data_config.parquet'\n",
    "\n",
    "# Use pandas to read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "df\n",
    "\n",
    "# Now, you can work with the DataFrame 'df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367b466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ff96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef8df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c44442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f73305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                         # Modify this part to write the DataFrame in Parquet format\n",
    "#                         output_dir = self.config[\"output\"][\"dir\"]\n",
    "#                         output_filename = file_name.replace(\".csv\", \".parquet\")  # Update the file extension\n",
    "#                         output_path = f\"{output_dir}/{output_filename}\"\n",
    "#                         result_dataframe.to_parquet(output_path, index=False)  # Write DataFrame to Parquet\n",
    "                        \n",
    "                   \n",
    " \n",
    "#             for function in function_name:\n",
    "#                 params = {}\n",
    "#                 print(function)\n",
    "                \n",
    "#             for function_entry in self.config[\"function\"]:\n",
    "                    \n",
    "#                 function_name = function_entry[\"name\"]\n",
    "#                 if function_name in input_paths[\"file\"][\"functions\"]:\n",
    "#                     print(f\"Function Name: {function_name}\")\n",
    "\n",
    "#                 # If \"params\" is present, you can access its values like this\n",
    "#                 params = function_entry.get(\"params\", {})\n",
    "#                 print(f\"Parameters: {params}\")\n",
    "#                 param_values = params.values()\n",
    "#                 print(f\"Parameter Values: {param_values}\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f379523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3e8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "for function_name in functions:\n",
    "                if hasattr(self, function_name):\n",
    "                    function = getattr(self, function_name)\n",
    "                    if function_name == 'creating_unique_key':\n",
    "                        key_columns = file_config.get('params', {}).get('key_columns', [])\n",
    "                        print(\"Configured key_columns:\", key_columns)\n",
    "                        print(\"before\")\n",
    "                        print(dataframe)\n",
    "                        dataframe = function(dataframe, key_columns)\n",
    "                        print(\"after\")\n",
    "                        print(dataframe)\n",
    "                    elif function_name == 'find_missing_dates':\n",
    "                        date_freq = file_config.get('params', {}).get('date_freq', 'D')  # Default to 'D' if not specified in the config\n",
    "                        function(dataframe, date_freq)\n",
    "                    elif function_name == 'validate_minimum_timeperiod_value':\n",
    "                        min_time_period = file_config.get('params', {}).get('min_time_period', 100)  # Default to 100 if not specified in the config\n",
    "                        function(dataframe, min_time_period)\n",
    "                    else:\n",
    "                        function(dataframe)\n",
    "\n",
    "                else:\n",
    "                    self.create_alert(message=f\"Function '{function_name}' not found.\", alert_type=ALERT.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b654dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae57a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaaaaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "# Enum for alert types\n",
    "class ALERT(Enum):\n",
    "    WARNING = 1\n",
    "    ERROR = 2\n",
    "\n",
    "# Alert class to represent alerts and warnings\n",
    "class Alert:\n",
    "    def __init__(self, timestamp, stage, substage=None, message=\"\", alert_type=ALERT.WARNING):\n",
    "        self.timestamp = timestamp\n",
    "        self.stage = stage\n",
    "        self.substage = substage\n",
    "        self.message = message\n",
    "        self.alert_type = alert_type\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{self.timestamp}\\t\"\n",
    "            f\"{self.alert_type}\\t\"\n",
    "            f\"{self.stage}\\t\"\n",
    "            f\"{self.substage}\\t\"\n",
    "            f\"{self.message}\"\n",
    "        )\n",
    "\n",
    "class Stage():\n",
    "    def __init__(self, config_path) -> None:\n",
    "        # Load the configuration from JSON file\n",
    "        with open(config_path, \"r\") as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_stage(self):\n",
    "        pass\n",
    "    \n",
    "    # Method to create and print an error\n",
    "    def create_alert(self, substage=None, message=\"\", alert_type=ALERT.WARNING):\n",
    "        alert = Alert(datetime.now(), self.config['stage_name'], substage, message, alert_type)\n",
    "        print(str(alert))\n",
    "\n",
    "\n",
    "# Data validation class\n",
    "class DataValidation(Stage):\n",
    "\n",
    "    def run_stage(self):\n",
    "        # Load input data\n",
    "        input_dir = self.config['input']['dir']\n",
    "        input_files = self.config['input']['file']\n",
    "        input_function = self.config['function']\n",
    "\n",
    "        for file_config in input_files:\n",
    "            file_name = file_config.get('calendar') or file_config.get('time series')\n",
    "            file_format = self.config['input']['fileFormat']\n",
    "            file_path = f\"{input_dir}/{file_name}.{file_format}\"\n",
    "\n",
    "            dataframe = pd.read_csv(file_path, index_col=0)\n",
    "            dataframe.name = file_config.get('calendar') or file_config.get('time series')  # Set the DataFrame name\n",
    "\n",
    "            functions = file_config.get('functions', [])\n",
    "            print(\"File config:\", file_config)\n",
    "            print(\"Params section:\", input_function[1].get('params', {}))\n",
    "\n",
    "            for function_name in functions:\n",
    "                \n",
    "                function_config = next((func for func in input_function if func['name'] == function_name), {})\n",
    "                params_section = function_config.get('params', {})\n",
    "                print(\"Params section for function\", function_name, \":\", params_section)\n",
    "        \n",
    "                if hasattr(self, function_name):\n",
    "                    function = getattr(self, function_name)\n",
    "                    if function_name == 'validate_date_format':\n",
    "                        print(\"before\")\n",
    "                        print(dataframe)\n",
    "                        dataframe = function(dataframe)\n",
    "                        print(\"after\")\n",
    "                        print(dataframe)\n",
    "                    elif dataframe.name == 'forecastra_sample':\n",
    "                        if function_name == 'creating_unique_key':\n",
    "                            #key_columns = file_config.get('params', {}).get('key_columns', [])\n",
    "                            key_columns = params_section.get('key_columns', [])\n",
    "                            print(\"Configured key_columns:\", key_columns)\n",
    "                            print(\"before\")\n",
    "                            print(dataframe)\n",
    "                            dataframe = function(dataframe, key_columns)\n",
    "                            print(\"after\")\n",
    "                            print(dataframe)\n",
    "                           \n",
    "                        elif function_name == 'find_missing_dates':\n",
    "                            date_freq = file_config.get('params', {}).get('date_freq', 'D')\n",
    "                            function(dataframe, date_freq)\n",
    "                        elif function_name == 'validate_minimum_timeperiod_value':\n",
    "                            min_time_period = file_config.get('params', {}).get('min_time_period', 100)\n",
    "                            function(dataframe, min_time_period)\n",
    "\n",
    "#                 else:\n",
    "#                     self.create_alert(message=f\"Function '{function_name}' not found.\", alert_type=ALERT.ERROR)\n",
    "\n",
    "                    # Save output data\n",
    "                    output_dir = self.config['output']['dir']\n",
    "                    output_files = self.config['output']['file']\n",
    "\n",
    "                    for file_config in output_files:\n",
    "                        file_name = file_config.get('calendar') or file_config.get('time series')\n",
    "                        file_format = self.config['output']['fileFormat']\n",
    "                        file_path = f\"{output_dir}/{file_name}.{file_format}\"\n",
    "\n",
    "                        dataframe.to_parquet(file_path, index=False)\n",
    "    \n",
    "    # Function to create a unique key in the dataframe\n",
    "    def creating_unique_key(self, dataframe, key_columns):\n",
    "        print(\"Creating unique key with columns:\", key_columns)\n",
    "        if len(key_columns) == 1:\n",
    "            # Rename the single selected column as 'unique_id'\n",
    "            dataframe['unique_id'] = dataframe[key_columns[0]]\n",
    "            #key_series = dataframe['unique_id']\n",
    "            \n",
    "        else:\n",
    "            # Combine selected columns into a new 'unique_id' column\n",
    "            #key_series = dataframe[key_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
    "            dataframe['unique_id'] = dataframe[key_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
    "\n",
    "        #return key_series\n",
    "        return dataframe\n",
    "\n",
    "    # Function to validate the date format in the dataframe\n",
    "    def validate_date_format(self, dataframe):\n",
    "        if not pd.api.types.is_datetime64_any_dtype(dataframe['date']):\n",
    "            for date_str in dataframe['date']:\n",
    "                try:\n",
    "                    datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                except ValueError as e:\n",
    "                    self.create_alert(message=f\"Invalid date format: {date_str}. Please use the format YYYY-MM-DD.\")\n",
    "                    \n",
    "        return dataframe\n",
    "\n",
    "    # Function to find missing dates in the dataframe                    \n",
    "    def find_missing_dates(self, dataframe, date_freq='D'):\n",
    "        # Checking for missing dates\n",
    "\n",
    "        # Sort the dataframe by 'date'\n",
    "        dataframe['date'] = pd.to_datetime(dataframe['date']).astype('datetime64[ns]')\n",
    "        df = dataframe.sort_values('date')\n",
    "\n",
    "        # Generate a range of dates covering the desired time period\n",
    "        start_date = df['date'].min()\n",
    "        end_date = df['date'].max()\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=date_freq)\n",
    "\n",
    "        # Find missing dates\n",
    "        missing_dates = date_range[~date_range.isin(df['date'])]\n",
    "\n",
    "        # Generate warnings for missing dates\n",
    "        for missing_date in missing_dates:\n",
    "            self.create_alert(message=f\"Warning: Data is missing for date {missing_date.date()}\")\n",
    "\n",
    "    # Function to validate the minimum time period in the dataframe\n",
    "    def validate_minimum_timeperiod_value(self, dataframe, min_time_period=100):\n",
    "        # User to enter minimum time period value\n",
    "\n",
    "        if dataframe['date'].max() - dataframe['date'].min() < pd.Timedelta(days=min_time_period):\n",
    "            self.create_alert(message=f\"Warning: Data does not have a minimum time period of {min_time_period} days.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run a test stage\n",
    "    stage = DataValidation(config_path='D:/Forecastra Accelerator/dataset/.vscode/read_config.json')\n",
    "    stage.run_stage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3df1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04fda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de396e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610f7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a87bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e827bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "# Enum for alert types\n",
    "class ALERT(Enum):\n",
    "    WARNING = 1\n",
    "    ERROR = 2\n",
    "\n",
    "# Alert class to represent alerts and warnings\n",
    "class Alert:\n",
    "    def __init__(self, timestamp, stage, substage=None, message=\"\", alert_type=ALERT.WARNING):\n",
    "        self.timestamp = timestamp\n",
    "        self.stage = stage\n",
    "        self.substage = substage\n",
    "        self.message = message\n",
    "        self.alert_type = alert_type\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{self.timestamp}\\t\"\n",
    "            f\"{self.alert_type}\\t\"\n",
    "            f\"{self.stage}\\t\"\n",
    "            f\"{self.substage}\\t\"\n",
    "            f\"{self.message}\"\n",
    "        )\n",
    "\n",
    "class Stage():\n",
    "    def __init__(self, config_path) -> None:\n",
    "        # Load the configuration from JSON file\n",
    "        with open(config_path, \"r\") as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_stage(self):\n",
    "        pass\n",
    "    \n",
    "    # Method to create and print an error\n",
    "    def create_alert(self, substage=None, message=\"\", alert_type=ALERT.WARNING):\n",
    "        alert = Alert(datetime.now(), self.config['stage_name'], substage, message, alert_type)\n",
    "        print(str(alert))\n",
    "\n",
    "\n",
    "# Data validation class\n",
    "class DataValidation(Stage):\n",
    "\n",
    "    def run_stage(self):\n",
    "        # Load input data\n",
    "        input_dir = self.config['input']['dir']\n",
    "        input_files = self.config['input']['file']\n",
    "        input_function = self.config['function']\n",
    "\n",
    "        for file_config in input_files:\n",
    "            if 'calendar' in file_config:\n",
    "                #file_name = file_config['calendar']\n",
    "                file_name = file_config.get('calendar')\n",
    "            elif 'time series' in file_config:\n",
    "                #file_name = file_config['time series']\n",
    "                file_name = file_config.get('time series')\n",
    "            #file_name = file_config.get('calendar') or file_config.get('time series')\n",
    "            file_format = self.config['input']['fileFormat']\n",
    "            file_path = f\"{input_dir}/{file_name}.{file_format}\"\n",
    "\n",
    "            dataframe = pd.read_csv(file_path, index_col=0)\n",
    "            dataframe.name = file_config.get('calendar') or file_config.get('time series')  # Set the DataFrame name\n",
    "\n",
    "            functions = file_config.get('functions', [])\n",
    "            print(\"File config:\", file_config)\n",
    "            print(\"Params section:\", input_function[1].get('params', {}))\n",
    "\n",
    "            for function_name in functions:\n",
    "                \n",
    "                function_config = next((func for func in input_function if func['name'] == function_name), {})\n",
    "                params_section = function_config.get('params', {})\n",
    "                print(\"Params section for function\", function_name, \":\", params_section)\n",
    "        \n",
    "                if hasattr(self, function_name):\n",
    "                    function = getattr(self, function_name)\n",
    "                    if function_name == 'validate_date_format':\n",
    "                        print(\"before\")\n",
    "                        print(dataframe)\n",
    "                        dataframe = function(dataframe)\n",
    "                        print(\"after\")\n",
    "                        print(dataframe)\n",
    "                    if dataframe.name == 'forecastra_sample':\n",
    "                        if function_name == 'creating_unique_key':\n",
    "                            #key_columns = file_config.get('params', {}).get('key_columns', [])\n",
    "                            key_columns = params_section.get('key_columns', [])\n",
    "                            print(\"Configured key_columns:\", key_columns)\n",
    "                            print(\"before\")\n",
    "                            print(dataframe)\n",
    "                            dataframe = function(dataframe, key_columns)\n",
    "                            print(\"after\")\n",
    "                            print(dataframe)\n",
    "                           \n",
    "                        elif function_name == 'find_missing_dates':\n",
    "                            date_freq = file_config.get('params', {}).get('date_freq', 'D')\n",
    "                            function(dataframe, date_freq)\n",
    "                        elif function_name == 'validate_minimum_timeperiod_value':\n",
    "                            min_time_period = file_config.get('params', {}).get('min_time_period', 100)\n",
    "                            function(dataframe, min_time_period)\n",
    "\n",
    "                # Save output data\n",
    "                output_dir = self.config['output']['dir']\n",
    "                output_files = self.config['output']['file']\n",
    "\n",
    "                if dataframe.name == 'calendar_data':\n",
    "                    # Save the 'calendar' data\n",
    "                    for file_config in output_files:\n",
    "                        if 'calendar' in file_config:\n",
    "                            file_name = file_config.get('calendar')\n",
    "                            file_format = self.config['output']['fileFormat']\n",
    "                            file_path = f\"{output_dir}/{file_name}.{file_format}\"\n",
    "                            dataframe.to_parquet(file_path, index=False)\n",
    "\n",
    "                elif dataframe.name == 'forecastra_input_data':\n",
    "                    # Save the 'forecastra input' data\n",
    "                    for file_config in output_files:\n",
    "                        if 'time series' in file_config:\n",
    "                            file_name = file_config.get('time series')\n",
    "                            file_format = self.config['output']['fileFormat']\n",
    "                            file_path = f\"{output_dir}/{file_name}.{file_format}\"\n",
    "                            dataframe.to_parquet(file_path, index=False)\n",
    "\n",
    "#             for file_config in output_files:\n",
    "#                 file_name = file_config.get('calendar') or file_config.get('time series')\n",
    "#                 file_format = self.config['output']['fileFormat']\n",
    "#                 file_path = f\"{output_dir}/{file_name}.{file_format}\"\n",
    "\n",
    "#                 dataframe.to_parquet(file_path, index=False)\n",
    "    \n",
    "    # Function to create a unique key in the dataframe\n",
    "    def creating_unique_key(self, dataframe, key_columns):\n",
    "        print(\"Creating unique key with columns:\", key_columns)\n",
    "        if len(key_columns) == 1:\n",
    "            # Rename the single selected column as 'unique_id'\n",
    "            dataframe['unique_id'] = dataframe[key_columns[0]]\n",
    "            #key_series = dataframe['unique_id']\n",
    "            \n",
    "        else:\n",
    "            # Combine selected columns into a new 'unique_id' column\n",
    "            #key_series = dataframe[key_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
    "            dataframe['unique_id'] = dataframe[key_columns].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n",
    "\n",
    "        #return key_series\n",
    "        return dataframe\n",
    "\n",
    "    # Function to validate the date format in the dataframe\n",
    "    def validate_date_format(self, dataframe):\n",
    "        if not pd.api.types.is_datetime64_any_dtype(dataframe['date']):\n",
    "            for date_str in dataframe['date']:\n",
    "                try:\n",
    "                    datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                except ValueError as e:\n",
    "                    self.create_alert(message=f\"Invalid date format: {date_str}. Please use the format YYYY-MM-DD.\")\n",
    "                    \n",
    "        return dataframe\n",
    "\n",
    "    # Function to find missing dates in the dataframe                    \n",
    "    def find_missing_dates(self, dataframe, date_freq='D'):\n",
    "        # Checking for missing dates\n",
    "\n",
    "        # Sort the dataframe by 'date'\n",
    "        dataframe['date'] = pd.to_datetime(dataframe['date']).astype('datetime64[ns]')\n",
    "        df = dataframe.sort_values('date')\n",
    "\n",
    "        # Generate a range of dates covering the desired time period\n",
    "        start_date = df['date'].min()\n",
    "        end_date = df['date'].max()\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=date_freq)\n",
    "\n",
    "        # Find missing dates\n",
    "        missing_dates = date_range[~date_range.isin(df['date'])]\n",
    "\n",
    "        # Generate warnings for missing dates\n",
    "        for missing_date in missing_dates:\n",
    "            self.create_alert(message=f\"Warning: Data is missing for date {missing_date.date()}\")\n",
    "\n",
    "    # Function to validate the minimum time period in the dataframe\n",
    "    def validate_minimum_timeperiod_value(self, dataframe, min_time_period=100):\n",
    "        # User to enter minimum time period value\n",
    "\n",
    "        if dataframe['date'].max() - dataframe['date'].min() < pd.Timedelta(days=min_time_period):\n",
    "            self.create_alert(message=f\"Warning: Data does not have a minimum time period of {min_time_period} days.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run a test stage\n",
    "    stage = DataValidation(config_path='D:/Forecastra Accelerator/dataset/.vscode/read_config.json')\n",
    "    stage.run_stage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
